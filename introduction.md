# La Société automatique
## 1. L’Avenir du travail

> « Objectivité rationnelle, objectivité technique, objectivité sociale sont désormais trois caractères fortement liés. Si l’on oublie un seul de ces caractères de la culture scientifique moderne, on entre dans le domaine de l’utopie. »
> Gaston Bachelard

> « Inlassablement, nous édifions le monde, afin que la secrète dissolution, l’universelle corruption qui régit ce qui “est”, soit oubliée au profit de cette cohérence de notions et d’objets, de rapports et de formes, claire, définie, ouvrage de l’homme tranquille, où le néant ne saurait s’infiltrer et où de beaux noms - tous les noms sont beaux - suffisent à nous rendre heureux. »
> Maurice Blanchot

> « Ces moteurs doivent être très différents des autres. Il semble logique de supposer que Morel les a dessinés de manière que leur fonctionnement ne puisse être compris du premier venu qui débarque dans l’île. Sans doute, la difficulté de leur maniement doit-elle résider dans des différences avec les autres moteurs. Comme je n’entends rien à aucun, cette difficulté majeure disparaît pour moi... Et si Morel avait eu l’idée d’enregistrer les moteurs aussi ?... »
> Adolfo Bioy Casares

### Introduction

#### Entropie et néguentropie dans l'Anthropocène

> « Le plus étrange dans ce retour en fanfare de l’“espèce humaine” dans l’histoire est que l’Anthropocène fournit la démonstration la plus éclatante que, d’un point de vue environnemental, l’humanité prise comme un tout n’existe pas. »
> Christophe Bonneuil
>et Jean-Baptiste Fressoz

##### 1. Ce qui s’est passé entre les 23 juin et 23 octobre 2008

Le 23 juin 2008, en analysant dans *Wired* le modèle d’affaires de Google, Chris Anderson montra que tous les services offerts par cette entreprise - qui sont basés sur ce que Frédéric Kaplan a appelé depuis le *capitalisme linguistique*<sup id="a1">[1](#f1)</sup>. - sont réalisés sans aucune référence à une théorie du langage, quelle qu’elle soit<sup id="a2">[2](#f2)</sup>.

Partant de là, et tenant un raisonnement similaire en matière d’épidémiologie googlienne, il en vint à poser en principe que, avec ce qu’on appelle aujourd’hui les *big data*<sup id="a3">[3](#f3)</sup>, constitués par les milliards de données analysables en temps réel par le calcul intensif, il n’y a plus besoin ni de théorie, ni de théoriciens - comme si les *data « scientists »*, spécialistes des mathématiques appliquées à de très grandes bases de données par l’intermédiaire d’algorithmes, pouvaient se substituer aux théoriciens que sont toujours, en principe, les scientifiques, quels que soient les champs et les disciplines scientifiques
concernés.

Quatre mois plus tard, le 23 octobre 2008, Alan Greenspan, président de la Réserve fédérale jusqu’en 2000, fut auditionné à Washington par la Chambre des représentants : il s’agissait pour lui d’expliquer les raisons pour lesquelles tant de catastrophes financières s’étaient enclenchées à partir de la crise des *subprimes* en août 2007. Mis en cause pour n’avoir pas su anticiper ni prévenir la crise systémique, il se défendit en affirmant que la cause en était le mésusage des mathématiques financières et les systèmes de calcul automatisé supportant l’évaluation des risques, et instaurés par le *digital trading* sous ses diverses formes (des *subprimes au high frenquency trading*) :

> C’est l’échec à évaluer correctement les actifs risqués qui a précipité la crise. Au cours des dernières décennies, un vaste système de gestion et d’appréciation des risques s’est développé, combinant les meilleures idées de mathématiciens et d’experts financiers soutenus par des avancées majeures dans les technologies informatiques et les télécommunications<sup id="a4">[4](#f4)</sup>.

Greenspan souligna en outre que le « prix Nobel d’économie » avait légitimé ces approches<sup id="a5">[5](#f5)</sup> :  - ce qui signifiait que, s’il devait y avoir une mise en cause, elle ne pouvait se limiter au seul président de la Réserve fédérale des Etats-Unis d’Amérique : elle concernait tout l’appareil de formalisation computationnelle et de décisions automatiques afférentes prises par les robots financiers, ainsi que la « théorie » économique occulte qui en avait soutenu la légitimité.

Si, jusqu’en août 2007, cela avait « marché » (« ce paradigme a régné pendant des décennies »), si l’appareil de formalisation computationnelle et de décisions automatiques s’était imposé *de fait*,

> tout cet édifice intellectuel s’est cependant effondré l’été de l’année dernière [2007] parce que les données entrées dans les modèles de gestion des risques ne provenaient généralement que des deux dernières décennies, qui furent une période d’euphorie.

Sans doute les idéologues de cette « gestion rationnelle des risques » n’ignoraient-ils pas que leurs « jeux de données » étaient limités, ajouterais-je au propos de Greenspan. Mais ils faisaient l’hypothèse que les « périodes historiques de stress » avaient eu lieu *parce que ces instruments financiers n’existaient pas durant ces périodes*, et que la concurrence n’était pas encore « parfaite et non faussée ». Telle était la théorie cachée derrière ces robots supposés « objectiver » le réel, qui accompliraient donc la « rationalité des marchés ».

Peu de temps après l’article de Chris Anderson, Kevin Kelly lui objecta que, derrière toute appréhension automatisée des faits, il y a une théorie cachée, connue ou inconnue et, dans ce dernier cas, *en attente de formulation*<sup id="a6">[6](#f6)</sup>. Pour nous, sinon pour Kelly lui-même, cela veut dire que *derrière et au-delà de tout fait, il y a un droit*.

La science, c’est ce qui va au-delà des faits en *excipant* de ce droit : c’est ce qui pose qu’il y a *toujours une exception* (et c’est ce que signifie « exciper » en droit : affirmer un droit de l’exception) à la *majorité* des faits, voire à l'*immense majorité* des faits, c’est-à-dire à leur quasi-totalité, et *qui les invalide en droit* (qui invalide leur cohérence apparente). C’est ce que, dans les chapitres qui suivent, nous appellerons avec Yves Bonnefoy et Maurice Blanchot l’improbable - et telle est aussi la question du cygne noir posée par Nassim Nicholas Taleb en un sens plus proche de l’épistémologie des statistiques, des probabilités et de la catégorisation<sup id="a7">[7](#f7)</sup>.

##### 2. Mettre Paris en bouteille

L’idéologie de la concurrence parfaite et non faussée était et reste aujourd’hui encore le discours du néolibéralisme, y compris pour Greenspan, qui concluait en ce sens, le 23 octobre 2008, en posant que 

> si les modèles avaient été élaborés en fonction de périodes historiques de stress, les exigences de fonds propres [détenus par les établissements financiers] auraient été beaucoup plus élevées, et le monde financier serait, à mon avis, en bien meilleure forme aujourd’hui.

Mais ce qu’il occulte par cette remarque, c’est qu’« avec des *si*, on mettrait Paris en bouteille ». Car si ces exigences de fonds propres avaient été « beaucoup plus élevées », le modèle ne se serait tout simplement pas développé : il avait précisément pour fonction essentielle de dissimuler l’insolvabilité systémique du *consumer capitalism* (c’est-à-dire de la « croissance »), frappé depuis plus de trente ans par une réduction drastique du pouvoir d’achat des salariés, réduction imposée par la révolution conservatrice - et par la financiarisation, en quoi celle-ci consista fondamentalement, ce qui permit aussi d’organiser l’endettement structurel des États et leur soumission à un chantage sans précédent, qu’il faut apparenter au racket (c’est pourquoi on peut parler de capitalisme mafieux<sup id="a8">[8](#f8)</sup>).

L’application de ce modèle fondé sur l’« industrie financière » et ses technologies computationnelles automatisées avait pour but à la fois de capter *sans redistribution* les plus-values dégagées par les gains de productivité et de dissimuler par une cavalerie financière, assistée par ordinateur, à l’échelle planétaire que la révolution conservatrice avait cassé le « cercle vertueux » du « compromis » fordo-keynésien<sup id="a9">[9](#f9)</sup>.

C’est ainsi que, avec la révolution conservatrice, le capitalisme devint *purement computationnel* (sinon « purement mafieux »). En 1905, Max Weber avait montré, d’une part, que le capitalisme se rapportait originellement à un incalculable dont le symbole était le Christ comme clé de voûte de l’éthique protestante, elle-même constituant l’esprit du capitalisme<sup id="a10">[10](#f10)</sup>, et d’autre part, que la dynamique transformatrice de la société que cet « esprit » avait installée consistait en une sécularisation et une rationalisation qui le contrariaient irrépressiblement - ce que l’on pourrait appeler *l’aporie du capitalisme*<sup id="a1">[11](#f11)</sup>.

Nous verrons que le devenir *purement* computationnel du capitalisme contemporain, concrétisé par l’économie dite des *data*, qui exacerbe cette aporie, réalise cette contradiction, et, ce faisant, accomplit ce devenir sans avenir que Nietzsche avait nommé le *nihilisme* - ce dont les affirmations tapageuses d’Anderson et les explications embrouillées de Greenspan sont des symptômes (au sens que Paolo Vignola donne à ce mot<sup id="a12">[12](#f12)</sup>).

##### 3. Ce que nous cache Quelle France dans dix ans ?

Le *storytelling* pratiqué par Anderson relève d’une nouvelle idéologie qui a pour but de (se) masquer que, avec l’automatisation totale, une nouvelle explosion d’insolvabilité généralisée se prépare, bien pire que celle de 2008 - l’automatisation devant caractériser les dix prochaines années selon de nombreuses études, conjectures et « notes de conjoncture ».

Le 13 mars 2014, Bill Gates déclarait à Washington qu’avec la *software substitution*, c’est-à-dire avec la généralisation des robots logiques et algorithmiques pilotant des robots physique - des *smart cities* à Amazon en passant par les usines Mercedes, le métro et les camions livrant des supermarchés d’où les caissières auront disparu tout comme les manutentionnaires, sinon les clients -, l’emploi allait drastiquement diminuer au cours des vingt prochaines années, au point de devenir une situation exceptionnelle.

Cette thèse, qui est explorée depuis quelques années en profondeur<sup id="a13">[13](#f13)</sup>, a été récemment répercutée dans la presse européenne, d’abord en Belgique par le quotidien *Le Soir*, qui annonçait en juillet 2014 le risque d’une destruction de 50 % des emplois dans ce pays « d’ici une à deux décennies », puis en France : elle a été reprise par *Le Journal du dimanche* en octobre 2014, dans un article annonçant sur la base d’une étude commandée par ce journal au cabinet Roland Berger la destruction d’ici à 2025 de trois millions d’emplois touchant tout autant les classes moyennes, les emplois d’encadrement et les professions libérales que les métiers manuels. Notons que trois millions d’emplois perdus, cela repré- sente environ 11 points supplémentaires de chômage - c’est-à-dire un chômage complet à 24 % ne tenant pas compte des chômeurs étant employés « partiellement » ou « occasionnellement ».

En France, dans dix ans, quels que soient son niveau et les façons de le comptabiliser, le chômage oscillera vraisemblablement entre 24 % et 30 % (le scénario de Roland Berger étant relativement optimiste par rapport aux prévisions de l’institut Bruegel, comme on va le voir *infra*), et, dans tous les cas, ces études annoncent la disparition définitive du modèle fordo-keynésien qui organisait la redistribution des gains de productivité issus de l’automatisation taylorienne sous forme de pouvoir d’achat acquis à travers les salaires.

C’est une *immense transformation* qui s’annonce ainsi. Or, le rapport remis par Jean Pisani-Ferry durant l’été 2014 au président de la République française, qui le lui avait commandé en vue d’un « séminaire gouvernemental », ne dit *pas un mot* de ces perspectives littéralement bouleversantes - qui bouleverseront toute la macro-économie à venir.

*Quelle France dans dix ans ?* parle évidemment de l’emploi, mais sur un ton patelin, en disant en quelque sorte : « Soyons modestes,
fixons-nous un objectif réaliste : tenons-nous en matière d’emploi dans le premier tiers des performances des pays comparables<sup id="a14">[14](#f14)</sup>. » Et tout à l’avenant, durant plus de deux cent pages d’eau tiède qui ne disent pas un mot des hypothèses de réduction drastique de l’emploi, et qui tout au contraire commence par affirmer que 

> le but doit être le plein emploi. Aussi lointain qu’il paraisse aujourd’hui, c’est l’état normal de fonctionnement d’une économie. Tout autre état social revêt un caractère pathologique et implique un insupportable gâchis de compétences et de talents. Il n’y a aucune raison de renoncer à l’atteindre, alors que nous avons longtemps connu une situation de très faible chômage et
que certains de nos proches voisins y sont aujourd’hui revenus<sup id="a15">[15](#f15)</sup>.

Selon le commissaire général de France Stratégie<sup id="a16">[16](#f16)</sup>, il faudrait donc réaffirmer le plein emploi, mais il faudrait le faire de façon « crédible » - à travers un raisonnement cependant extraordinairement filandreux :

> Nous donner aujourd’hui cet objectif pour 2025 serait cependant jugé peu crédible par tous les Français qui souffrent depuis des décennies de la persistance d’un chômage de masse. Une ambition perçue à tort ou à raison comme trop élevée peut être démobilisatrice. Mieux vaut, comme le dit le proverbe chinois, traverser la rivière en sentant les pierres. En outre, le défaut d’un objectif formulé en termes absolus est de ne pas tenir compte de la situation économique globale et européenne. Le raisonnement en termes relatifs évite cet écueil. Dans cet esprit, nous pouvons ambitionner de revenir durablement dans le premier tiers des pays européens pour l’emploi<sup id="a17">[17](#f17)</sup>.

Au contraire de ce que prétend *Quelle France dans dix ans ?*, l’institut Bruegel, dont Pisani-Ferry fut le directeur (et un membre fondateur) avant sa nomination comme commissaire général de France Stratégie, soutient en la personne de Jeremy Bowles et en reprenant les chiffres de Benedikt Frey et Michael Osborne à Oxford Martin School<sup id="a18">[18](#f18)</sup> que la Belgique pourrait perdre 50 % de ses emplois, l’Angleterre 43 %, l’Italie et la Pologne 56 % - tout cela, d’après *Le Soir*, « d’ici une à deux décennies ».

Au moment où il remit son rapport (en juin 2014), Pisani-Ferry ne pouvait ignorer ces calculs de l’institut qu’il a contribué à créer en 2005. Comment a-t-il pu s’autoriser à les dissimuler ? La réalité est qu’il a, comme Greenspan, intériorisé un état de fait calamiteux dont il entretient la mécompréhension à travers une analyse profondément erronée, empêchant la France de prendre la mesure d’une situation exceptionnellement périlleuse, où, comme les

> caissière, nounou, contrôleur ou encore prof [...], d’ici à 2025, un tiers des emplois pourraient être occupés par des machines, des robots ou des logiciels dotés d’intelligence artificielle et capable d’apprendre par eux-mêmes. Et de nous remplacer. Une vision de l’avenir prophétisée par Peter Sondergaard, vice-président senior et directeur mondial de la recherche de Gartner<sup id="a19">[19](#f19)</sup>.

Nous verrons que cette « vision » est partagée par des dizaines d’analystes du monde entier - dont le cabinet Roland Berger qui soutient dans son étude que 

> d’ici 2025, 20 % des tâches seront automatisées. Et plus de 3 millions de salariés se trouveraient dépourvus d’emploi au profit des machines. Une liste sans fin de secteurs est concernée :
l’agriculture, l’hôtellerie, les administrations publiques, l’armée
et la police<sup id="a20">[20](#f20)</sup>...

Cacher de telles perspectives est très grave, comme le note un des associés de Roland Berger, Hakim El Karoui :

> « Le système fiscal n’est pas adapté pour prélever une partie de la richesse engendrée (par le numérique), l’effet de redistribution est donc très limité. »

Mettant en garde contre un risque de déflagration sociale, l’associé chez Roland Berger appelle à « anticiper, qualifier, dire la vérité [...], créer un électrochoc dans l’opinion dès maintenant ». Sous peine de voir la défiance envers les élites se renforcer, avec des impacts politiques graves<sup id="a21">[21](#f21)</sup>.

##### 4. Entropie et néguentropie dans 1’Anthropocène

Anticiper, qualifier, alerter, mais aussi proposer, tels sont les buts du présent ouvrage, qui envisage une façon tout à fait alternative de « redistribuer la richesse engendrée par le numérique », selon les termes de Hakim El Karoui. Y a-t-il un autre avenir, un recommencement possible dans le *processus d’automatisation intégrale et généralisée auquel aboutit la réticulation numérique planétaire* ?

Il faut poser cette question comme celle du passage de l’Anthropocène, qui installa les conditions de la prolétarisation généralisée dès la fin du XVIIIe siècle, ainsi qu’Adam Smith le comprenait déjà, à la sortie de cette période où l’anthropisation est devenue un « facteur géologique<sup id="a22">[22](#f22)</sup> ». Nous appellerons cette sortie le *Néguanthropocène*. La sortie de l’Anthropocène constitue l’horizon global des thèses avancées ici. Ces thèses posent en principe premier que *le temps gagné par l’automatisation doit être investi dans de nouvelles capacités de désautomatisation*, c’est-à-dire de *production de néguentropie*.

Depuis des décennies, la fin du travail salarié est ce dont des analystes tels Norbert Wiener aux Etats-Unis ou Georges Friedmann en France ont annoncé la disparition prochaine après John Maynard Keynes lui-même. Marx avait exploré cette hypothèse en profondeur dans un célèbre fragment des *Fondements de la critique de l'économie politique*, appelé le « fragment sur les machines » ou le « chapitre sur l’automatisation ».

Cette perspective va se concrétiser au cours de la prochaine décennie. Que ferons-nous dans les dix ans qui viennent pour tirer le meilleur parti possible de cette *immense transformation* ?

William Gates, après avoir lui-même annoncé le déclin de l’emploi, recommande de réduire les salaires et de suspendre les taxes et charges diverses qui y sont liées. Baisser une fois de plus les salaires de ceux qui auront encore un emploi ne pourrait pourtant qu’accroître l’insolvabilité globale du système capitaliste. La question est évidemment ailleurs : *le temps libéré par la fin de l'emploi doit être mis au service d’une culture des automates capable de produire une nouvelle valeur et de réinventer le travail*<sup id="a23">[23](#f23)</sup>. La *culture de la désautomatisation rendue possible par l'automatisation* est ce qui peut et ce qui doit produire de la valeur néguentropique - et elle requiert ce que j’avais nommé autrefois un *otium* du peuple<sup id="a24">[24](#f24)</sup>.

L’automatisation, telle qu’elle a été conduite depuis le taylorisme, a engendré une immense entropie, à une échelle telle qu’aujourd’hui, dans le monde entier, l’humanité doute fondamentalement de son avenir - et la jeunesse plus encore. Le doute de l’humanité sur son avenir et face au désœuvrement sans précédent de sa jeunesse est apparu au moment où l’Anthropocène, qui a commencé avec l’industrialisation, est devenu « conscient de lui-même ».

> Succédant à l’Holocène, période de 11 500 ans marquée par une relative stabilité climatique [...] qui a vu l’éclosion de l’agriculture, des villes, des civilisations, l’Anthropocène [...] a débuté avec la révolution industrielle. Nous sommes alors bien entrés dans un nouvel âge géologique de la Terre. Sous l’emprise de l’agir humain, « la Terre opère actuellement sous un état sans analogue antérieur » (Paul Crutzen et Will Steffen, « How Long Have We Been in The Anthropocene Era ? »)<sup id="a25">[25](#f25)</sup>.

Que l’Anthropocène soit devenu « conscient de lui-même<sup id="a26">[26](#f26)</sup>», cela signifie que les hommes sont devenus plus ou moins conscients d’appartenir à cette ère de l’Anthropocène, au sens où ils s’en sont sentis « responsables<sup id="a27">[27](#f27)</sup>» - et cela s’est produit au cours des années 1970 : après la Seconde Guerre mondiale et l’accélération qu’elle a provoquée dans l’Anthropocène, une « conscience commune » d’être un facteur géologique et la cause collective d’une entropisation massive et accélérée à travers l’anthropisation de masse serait apparue avant même que le concept d’Anthropocène n’ait été formulé (en 2000) -, ce que Bonneuil et Fressoz mettent en relief en se référant à un discours prononcé par Jimmy Carter, président des Etats-Unis, en 1979 :

> Notre identité n’est plus définie par ce que nous réalisons, mais par ce que nous possédons [...]. Consommer ne satisfait plus notre recherche de sens, nous avons appris que l’accumulation
des biens matériels ne peut combler nos existences vides de sens<sup id="a28">[28](#f28)</sup>.

Il est frappant qu’un président des Etats-Unis énonce la fin de l'*American way of life*. Bonneuil et Fressoz rappellent que c’est contre ce discours qu’est apparu Reagan,

> favorable à une restauration de l’hégémonie états-unienne et aux dérégulations des activités polluantes, [...] [cependant que] le discours de Carter illustre l’influence [...] qu’avait acquise dans l’espace public la critique de la société de consommation. 

Au cours des dernières années, et plus encore après 2008, cette « conscience de soi » de l’Anthropocène aura mis en évidence le caractère *systémiquement et massivement toxique* de l’organologie contemporaine<sup id="a29">[29](#f29)</sup> (outre qu’insolvable) au sens qu’Ars Industrialis et l’institut de recherche et d’innovation attribuent à ce terme dans la perspective d’une organologie générale<sup id="a30">[30](#f30)</sup>.

À partir de cette toxicité pharmacologique devenant une conscience commune au sens où ce que l’on croyait être tel ou tel facteur de progrès paraît inverser son signe et précipiter l’humanité dans la régression généralisée, l’Anthropocène, dont l’histoire est celle du capitalisme, se présente comme un processus commençant avec l’industrialisation organologique (également dans les pays réputés «anti-capitalistes»), c’est-à-dire avec la révolution industrielle - qu’il faut appréhender en conséquence comme une *révolution organologique*.

##### 5. L’accomplissement du nihilisme et l’entrée dans le Néguanthropocène

L’ère de l’Anthropocène, c’est l’ère du capitalisme industriel au sein duquel *le calcul prévaut sur tout autre critère de décision* et où, devenant algorithmique et machinique, il se concrétise et se matérialise comme automatisme logique, et constitue ainsi précisément l’avènement du nihilisme comme société computationnelle devenant automatique, téléguidée et télécommandée.

La confusion et le désarroi dans lequel nous laisse le stade dit « réflexif » parce que supposément « conscientisé » de l’Anthropocène sont un résultat historique au sein duquel on peut à présent identifier de nouveaux facteurs de causalité et de quasi-causalité qui n’ont guère été analysés jusqu’à présent. C’est pourquoi Bonneuil et Fressoz peuvent à juste titre déplorer l’approche « géocratique » qui court-circuite les analyses politiques de l’histoire ouverte par ce qu’ils appellent l'événement Anthropocène<sup id="a31">[31](#f31)</sup>.

Au point de vue historique et politique défendu par Bonneuil et Fressoz, il faut cependant ajouter que, à travers cet événement, ce qui fut structurellement dénié par la philosophie durant des siècles est devenu patent, à savoir que l’artefact est le ressort de l’hominisation, sa condition et son destin. Nul ne peut plus l’ignorer : ce que Valéry, Husserl et Freud posèrent entre les deux guerres mondiales comme un nouvel âge de l’humanité, c’est-à-dire comme sa conscience (et son inconscience) pharmacologique du « monde de l’esprit<sup id="a32">[32](#f32)</sup> », c’est ce qui est devenu une conscience (et une inconscience) *commune, brouillée et malheureuse*. Tel est le mal-être dans l’Anthropocène contemporain.

Il en résulte la nécessité impérative de requalifier le fait noétique en totalité - c’est-à-dire dans tous les champs du savoir (vivre, faire, concevoir) - et en y intégrant les points de vue d’André Leroi-Gourhan et de Georges Canguilhem, qui furent les premiers à poser l’artificialisation du vivant comme point de départ de l’hominisation<sup id="a33">[33](#f33)</sup>. Cet impératif se présente comme une situation d’extrême urgence vitale à la fois politique, économique et écologique. Et il pose en cela une question d'*organologie pratique*, c’est-à-dire de productions *inventives*.

Nous soutenons que cette question et ces productions passent (et nous montrerons ici pourquoi) par une réinvention du *World Wide Web* en totalité - *l'Anthropocène étant entré depuis 1993 dans une nouvelle époque* avec l’apparition du *Web* qui est à notre temps ce que les chemins de fer furent au début de l’Anthropocène.

C’est avec Nietzsche qu’il faut penser l’Anthropocène comme l’ère géologique en quoi consiste la dévaluation de toutes les valeurs : c’est dans l’Anthropocène, et comme son enjeu vital, que la tâche de tout savoir noétique devient la transvaluation des valeurs, au moment où l’âme noétique est confrontée à son automise en question organologique comme *l’accomplissement du nihilisme* constituant *l’épreuve même de notre temps* — comme l’Anthropocène concrétisé en tant qu’âgé du capitalisme se planétarisant.

C’est avec Nietzsche que, après l’événement Anthropocène, il faut penser l’avènement du Néguanthropocène, et le penser comme la transvaluation du devenir en avenir. Pour ce faire, il faut lire Nietzsche avec Marx tel que celui-ci pense le nouveau statut du savoir dans le capitalisme et l’avenir du travail en 1857 dans le fragment dit « sur les machines » ou « sur l’automatisation », où il est aussi question du *General Intellect*.

Lire ensemble Marx et Nietzsche au service d’une nouvelle critique de l’économie politique, où *l’éco-nomie est devenue un facteur d’échelle localement cosmique (une dimension du cosmos)* et donc une *éco-logie*, c’est ce qui doit conduire à un processus de *transvaluation* tel que les *valeurs économiques* aussi bien que les *dévalorisations morales* qu’elles ont provoquées lorsque le nihilisme s’est déchaîné comme consumérisme sont *« transvaluées » par la nouvelle valeur de toute valeur, c’est-à-dire : par la néguentropie* — ou entropie négative<sup id="a34">[34](#f34)</sup>, ou anti-entropie<sup id="a35">[35](#f35)</sup>.

Issue de la thermodynamique environ trente années après l’avènement de la technologie industrielle et le début de la *révolution organologique* qui est à l’origine de l’Anthropocène à la fois avec la grammatisation des gestes par les premiers automatismes industriels et avec la machine à vapeur<sup id="a36">[36](#f36)</sup>, la théorie de l’entropie requalifie la question de la valeur s’il est vrai que le *rapport entropie/néguentropie est la question vitale par excellence*. C’est selon de telles perspectives qu’il faut penser organologiquement et pharmacologiquement à la fois ce que l’on devrait appeler l'*entropocène* et la *néguanthropologie*.

##### 6. La question du feu et l’avènement de la thermodynamique

Le *kosmos* est pensé à l’aube de la philosophie comme identité et comme *équilibre*. Dans cette opposition posée en *principe* entre l’équilibre de l’origine ontologique et le déséquilibre des êtres corruptibles, la technique qui constitue la condition organologique est rapportée au sublunaire en tant que monde de la contingence et de « ce qui peut être autrement qu’il n’est » (*to endekhomenon allôs ekhein*), et se trouve en cela exclue de la pensée.

Or, l’Anthropocène rend une telle position intenable et constitue en conséquence une crise épistémique d’ampleur jamais égalée : l’avènement de la *machine* thermodynamique, qui a fait apparaître le monde humain comme *perturbation fondamentale*<sup id="a37">[37](#f37)</sup>, inscrit la processualité, l’irréversibilité du devenir et l’instabilité des équilibres en quoi tout cela consiste au cœur de la physique elle-même. Tous les *principes* de la pensée aussi bien que de l’action en sont bouleversés.

La machine thermodynamique, qui pose en *physique* le problème spécifique et nouveau de la dissipation de l’énergie, est aussi l’objet technique industriel qui perturbe fondamentalement les organisations *sociales*, altérant par là même radicalement « la compréhension que l’être-là a de son être<sup id="a38">[38](#f38)</sup> » et instaurant l’ère de ce que l’on appellera la « technoscience ». Tel qu’il consiste essentiellement en une *combustion*, cet objet technique, dont le régulateur à boule sera un élément capital au cœur de la conception cybernétique, introduit *la question du feu et de sa pharmacologie* à la fois sur le plan de l’astrophysique (qui a remplacé la cosmologie) et sur le plan de l’écologie humaine.

La question du feu - c’est-à-dire de la combustion - s’inscrit ainsi à la fois du point de vue de la physique et du point de vue de l’écologie anthropologique au cœur d’une pensée renouvelée du cosmos *en tant que cosmos* (et au-delà de la « cosmologie rationnelle » telle que Kant la pense encore<sup id="a39">[39](#f39)</sup>) : l’époque de l’Anthropocène ne peut apparaître comme telle qu’à partir du moment où la question du cosmos devient elle-même la question de la combustion, en thermodynamique comme en astrophysique - et en rapport, via la machine à vapeur, avec ce *pharmakon* éminent qu’est le feu domestique comme l’artifice par excellence que Prométhée apporte aux mortels, et sur quoi veille Hestia<sup id="a40">[40](#f40)</sup>.

Comme problème physique, la conquête technologique du feu<sup id="a41">[41](#f41)</sup> met l’anthropogenèse - c’est-à-dire l’organogenèse organologique, et pas seulement organique - au cœur de ce qu’Alfred N. Whitehead nomme *concrescence*, et comme *technicisation locale du cosmos*. Cette technicisation locale est relative, mais elle conduit à *concevoir le cosmos en totalité à partir de cette position* et à partir de cette *ouverture locale de la question du feu* comme *pharmakon* dont il faut *prendre soin* — où la question de l'*énergie* (et de l'*energeia*) que recèle le feu (qui est aussi la lumière), posée à partir de la révolution organologique *et épistémologique* de la thermodynamique reconsidérée par Erwin Schrodinger, constitue *la matrice de la pensée de la vie aussi bien que de 1‘information, et comme jeu de l’entropie et de la néguentropie*.

Installant *la question* de l’entropie et de la néguentropie parmi les hommes comme le *problème crucial* de leur vie quotidienne aussi bien que de la vie en général, et, finalement, de l’univers en totalité pour toute forme de vie, la *technique* constitue la matrice de toute pensée de l’*oikos*, de l’habitat et de sa loi. N’est-il pas frappant d’un tel point de vue que, au moment même où Schrodinger donne à Dublin les conférences qui sont à l’origine de *Qu’est-ce que la vie ?*, Canguilhem affirme que l’âme noétique est une forme de vie technique qui *requiert de nouvelles conditions de la fidélité pour surmonter les chocs d'infidélité* que provoque ce que nous avons appelé le *double redoublement épokhal*<sup id="a42">[42](#f42)</sup> ?

##### 7. L’Anthropocène comme succession de chocs technologiques et le rôle néguanthropique du savoir

Ce que Canguilhem décrit comme l’infidélité du milieu technique<sup id="a43">[43](#f43)</sup>, c’est ce qui est rencontré comme *choc technologique épokhal* par les êtres organologiques et pharmacologiques que nous sommes en tant qu’individus noétiques - c’est-à-dire intellectuels et spirituels. Ce choc et cette infidélité sont ce dont procède fondamentalement ce que Simondon appelle le déphasage de l’individu. Le déphasage de l’individu par rapport à lui-même est le principe dynamique de l’individuation. 

Nous avons développé le concept de « double redoublement
épokhal » pour tenter de décrire comment un choc commence par
détruire des circuits de transindividuation établis, issus d’un choc
précédent, puis donne lieu à la génération de nouveaux circuits de
transindividuation<sup id="a44">[44](#f44)</sup>, qui constituent les nouveaux savoirs issus du
dernier choc. Une épokhè techno-logique est ce qui brise des auto-
matismes constitués, socialisés et capables de produire leurs propres
désautomatisations par des savoirs appropriés : la suspension des
automatismes socialisés (qui nourrissent la bêtise sous ses formes
si variées) se fait par la mise en place de nouveaux automatismes,
asociaux, dont le second temps du choc (comme second redou-
blement) produit de nouvelles capacités de désautomatisation,
c’est-à-dire de néguentropie, elle-même nourricière de nouvelles
organisations sociales.
Le savoir procède toujours d’un tel double choc - cependant
que la bêtise est toujours ce qui procède de l’automaticité.
Rappelons ici ce que Canguilhem pose en principe quant au
1.	Sur ce sujet, cf. Georges Canguilhem, Le Normal et le Pathologique, PUF, coll.
« Galien », 1966, et mon commentaire dans Ce qui fait que la vie vaut la peine d’être
vécue. De la pharmacologie, op. cit., p. 54.
2.	Sur la transindividuation et sur le transindividuel, Gilbert Simondon,
L’Individuation psychique et collective, Aubier, et B. Stiegler, La Télécratie contre la
démocratie, Flammarion, 2007, p. 120 et suivantes.
29
INTRODUCTION
sens plus-que-biologique de Vépistémè : la connaissance de la
vie est une forme spécifique de la vie conçue non seulement
comme biologie, mais comme connaissance des milieux, systèmes
et processus d'individuation, et où la connaissance est la condition
et l’avenir de la vie exposée aux chocs en retour de ses productions
techniques vitales (des productions organogénétiques qu’elle
sécrète pour compenser son défaut d’origine).
La connaissance est ce qui se constitue comme savoirs théra-
peutiques partagés des pharmaka en quoi consistent les organes
artificiels ainsi sécrétés. Elle est d’emblée sociale, et elle se
transindividue toujours plus ou moins en organisations sociales.
Connaissance des pharmaka, elle est aussi connaissance par les
pharmaka : elle est organologiquement constituée de part en
part, originellement extériorisée, mais tout aussi originellement
ntériorisée - faute de quoi ce n’est pas une connaissance, mais
tne information. C’est pourquoi elle ne se dilue pas dans la
« cognition » : les sciences cognitives, qui en sont une forme,
sont incapables de la penser (c’est-à-dire de se penser).
Il faut rapporter la fonction organo-logique de la connais-
sance telle que nous la comprenons à partir de Canguilhem, et
comme nécessitée par la forme technique de la vie, à ce que
Simondon dit de la connaissance de l’individuation : connaître
l’individuation, c’est l’individuer, c’est-à-dire que c’est déjà ne
plus la connaître parce que c’est la déphaser.
La connaissance comme savoir qui conditionne l’individua-
tion psychique aussi bien que collective du sachant « vient tou-
jours trop tard », comme dit Hegel, ce qui signifie qu’elle n’est
pas autosuffisante : elle suppose des savoir-vivre et -faire qui la
dépassent toujours et qui sont eux-mêmes toujours dépassés par
l’individuation technique qui engendre les chocs technologiques
constituant les époques des savoirs.
En se socialisant, la connaissance augmente la complexité
des sociétés qui l’individuent et en cela appartient à ce que
Whitehead nomme la concrescence du cosmos, lui-même conçu
30
LA SOCIÉTÉ AUTOMATIQUE
comme processus cosmique qui engendre des processus d’indi-
viduation où se rejouent chaque fois différemment les tendances
entropiques et néguentropiques.
À l’époque de l’Anthropocène, dont il s’agit de sortir au plus
vite, les questions de la vie et de la néguentropie venues de
Darwin et de Schrodinger doivent être requalifiées du point
de vue organologique défendu ici et qui est tel que :
1.	la sélection naturelle fait place à la sélection artificielle,
2.	le passage de l’organique à l’organologique déplace le jeu de
l’entropie et de la néguentropie1.
La technique est une accentuation de la néguentropie. C’est
un facteur de différenciation accrue : c’est « la poursuite de la
vie par d’autres moyens que la vie1 2 ». Mais c’est tout autant une
accélération de l’entropie, non seulement parce que c’est toujours
en quelque façon un processus de combustion et de dissipation
d’énergie, mais parce que la standardisation industrielle semble
conduire l’Anthropocène contemporain à la possibilité d’une
destruction de la vie comme buissonnement et prolifération
des différences - comme biodiversité, sociodiversité (« diversité
culturelle ») et psychodiversité des singularités engendrées par
défaut comme individuations psychiques aussi bien que comme
individuations collectives.
La destruction de la sociodiversité résulte des courts-circuits
des processus de transindividuation imposés par la standar-
disation industriehe. Nous verrons dans la conclusion de cet
1.	Ce qui ne peut qu’affecter radicalement la science écologique, et non seulement
la politique écologique, mais en inscrivant l’avènement politique au cœur même
de la science de ce vivant négociant avec du non-vivant organisé et avec les orga-
nisations qui en résultent.
2.	C’est le point de vue que j’ai défendu dans La Technique et le Temps 1. La faute
d’Épiméthée, op. cit., p. 146.
31
INTRODUCTION
ouvrage que l’anthropologie appréhendée comme entropologie
est le problème que Claude Lévi-Strauss rencontre sans parvenir
à le penser - sans parvenir à le poser comme la question de
la néguanthropologie, c’est-à-dire comme nouvelle époque du
savoir incarnant la tâche d’entrer dans le Néguanthropocène.
C’est ce qui conduit Lévi-Strauss à déserter la dimension poli-
tique de toute anthropologie.
L’Anthropocène est une époque organologique singulière en
ce qu’elle engendre la question organologique elle-même. C’est
ainsi quelle est rétroactivement constituée par sa propre recon-
naissance, où la question que pose l’Anthropocène est de sortir
de l’Anthropocène en tant que période toxique pour entrer
dans le Néguanthropocène comme époque curative et soigneuse
- économe en ce sens. Cela signifie pratiquement que dans le
Néguanthropocène, et sur le plan économique, l’accumulation
le valeur devra se faire exclusivement en vue d’investissements
que nous dirons néguanthropiques.
Nous appelons néguanthropique l’activité humaine telle qu’elle
est explicitement et impérativement gouvernée - à travers les
processus de transindividuation qu’elle met en œuvre, lesquels
résultent de critériologies établies par des dispositifs rétention-
nels - par des critères néguentropiques. La néguanthropisa-
tion du monde rompt avec l’anthropisation incurieuse de ses
effets entropiques - c’est-à-dire pour l’essentiel caractéristiques
de l’Anthropocène. Une telle rupture suppose le dépassement
de l’anthropologie telle que la concevait Lévi-Strauss par une
néguanthropologie qui est entièrement à élaborer.
La question de l’Anthropocène, qui porte en elle son propre
dépassement, et qui a la structure d’une promesse, émerge au
moment où s’instaure d’autre part l’automatisation intégrale
et généralisée provoquée par l’industrie des traces numériques
réticulaires, cependant que celle-ci paraît rendre cette promesse
intenable. Tenir bon, c’est-à-dire tenir cette promesse, c’est la
tenir précisément à partir des possibilités néguanthropiques
32
LA SOCIÉTÉ AUTOMATIQUE
ouvertes par l’automatisation : c’est penser cette industrie de
la réticulation comme une nouvelle époque du travail, comme
la fin de l’époque de l’emploi, que l’automatisation intégrale et
généralisée compromet à jamais, et comme « transvaluation »
de la valeur où
le temps de travail cesse et doit cesser d’être [la] mesure [du
travail], et [où] la valeur d’échange cesse donc aussi d’être la
mesure de la valeur d’usage1,
- où la valeur de la valeur devient la néguanthropie. Ce n’est
qu’ainsi que le passage de l’Anthropocène au Néguanthropocène
peut et doit s’accomplir.
8.	La smartification
Depuis 1993, un nouveau système technique planétaire se
met en place. Il est basé sur la rétention tertiaire numérique,
et il constitue l’infrastructure d’une société automatique à
venir. On nous dit que l’économie des data qui semble se
concrétiser comme dynamique économique engendrée par
cette infrastructure est le destin de cette société automatique
à venir.
Nous montrerons pourtant que le « destin » de cette société
d’hypercontrôle (chapitre un) n’a pas de destination : il ne mène
nulle part ailleurs qu’au nihilisme, c’est-à-dire à la négation
du savoir lui-même (chapitre deux). Et nous verrons d’abord
avec Jonathan Crary (chapitre trois), puis avec Thomas Berns
et Antoinette Rouvroy (chapitres quatre et cinq), pourquoi
cette société automatique à venir ne constituera un avenir
1.	Karl Marx, Grundrisse \_der Kritik der pôlitischen Ôkonomié] II ; Fondements de
la critique de l'économie politique. Anthropos, 1968, p. 221.
INTRODUCTION
— c’est-à-dire un destin dont la destination néguentropique est
le Néguanthropocène - qu’à la condition de dépasser cette
« économie des data », qui est en réalité la déséconomie d’une
« dissociété1 » (chapitre six).
L’actuel système d’exploitation industrielle des traces
modélisées et numérisées1 2 précipite la catastrophe entro-
pique qu’est l’Anthropocène comme destin qui ne mène
nulle part. Comme capitalisme 24/7 et gouvernementalité
algorithmique, il est hégémoniquement mis au service d’un
fonctionnement hyper-entropique qui accélère le rythme de
la destruction consumériste du monde tout en installant une
insolvabilité structurelle et insoutenable, basée sur une stupé-
faction généralisée et une functional stupidity destructrices
des capacités néguanthropologiques que recèlent les savoirs :
à la différence d’une simple compétence, qui ne sait pas ce
qu’elle fait, un savoir est un facteur cosmique intrinsèquement
néguentropique.
Nous entendons montrer dans cet ouvrage que l’infrastruc-
ture numérique réticulée qui supporte l’économie des data, dont
la mise en place advenue en 1993 avec le World Wide Web
constitue la dernière époque de l’Anthropocène, peut et doit
être renversée en une infrastructure néguanthropique fondée
sur une technologie digitale herméneutique mise au service de la
désautomatisation, c’est-à-dire basée sur l’investissement collectif
des gains de productivité issus de l’automatisation dans la culture
des savoir-faire, savoir-vivre et savoir-concevoir en tant qu’ils
sont par essence néguanthropiques et en cela producteurs d’une
1.	Jacques Généreux, La Dissociété, Le Seuil, 2006 ; édition revue et augmentée, Le
Seuil, coll. « Points», 2013.
2.	Le concept de M-traces, ou traces modélisées, est au centre de l’informatique
théorique contemporaine. Il a été développé en particulier par Alain Mille (CNRS-
Liris, université de Lyon) et par Yannick Prié (LINA, université de Nantes). Il a
également fait l’objet d’une publication, « De la trace à la connaissance à l’ère du
Web », MSH Paris-Nord, dans la revue Intellectica, n° 59, 2013.
34
LA SOCIÉTÉ AUTOMATIQUE
nouvelle valeur, seule capable d’instaurer l’ère porteuse d’une
nouvelle solvabilité que nous appelons le Néguanthropocène
(chapitres sept et huit).
L’infrastructure actuelle évolue à grands pas vers une société
d’hypercontrôle fondée sur les équipements mobiles, tel le
smartphone, les équipements domestiques, telle la télévision
connectée, les habitats, telles la smart house et la smart city, et
les équipements de transport, tehe l’automobile connectée.
Michael Price montrait le 30 octobre 2014 que la télévision
connectée est un instrument d’espionnage automatique des indi-
vidus :
Je viens d’acheter un nouveau téléviseur [...]. Je suis maintenant
le propriétaire d’un nouveau téléviseur «intelligent» [...]. Le
seul problème est que j’ai maintenant peur de l’utiliser. La quan-
tité de données que peut recueillir cet appareil est stupéfiante.
Il enregistre où, quand, comment et pour combien de temps
vous l’utilisez. Il définit les cookies et les balises de suivi visant
à détecter « quand vous avez vu tel contenu ou tel message
électronique ». Il enregistre « les applications que vous utilisez,
les sites que vous visitez, et comment vous interagissez avec le
contenu ». Il ignore la fonction « ne pas tracer », qu’il consi-
dère être une question politique. Il dispose également d’une
caméra intégrée de reconnaissance faciale. Le but est de fournir
le « contrôle gestuel » pour le téléviseur et de vous permettre
de vous connecter à un compte personnalisé en utilisant votre
visage1.
Qu’en sera-t-il avec les vêtements connectés qui apparaissent
à présent sur le marché1 2 ?
1.	Michael Price, «Je suis terrifié par ma nouvelle télé — et vous le seriez aussi»,
30 octobre, <http://www.salon.com/2014/10/30/im_terrified_of_my_new_tv_why_
im_scared_to_turn_this_thing_on_and_youd_be_too/>.
2.	Cf. Christophe Alix, « Des tee-shirts connectés franco-japonais à la fibre spor-
tive», Libération, 7 décembre 2014.
35
INTRODUCTION
Jérémie Zimmermann soulignait en outre dans un entretien
donné à Philosophie magazine en septembre 2013 que le smart-
phone a provoqué une véritable mutation dans le hardware de
l’infrastructure numérique, puisque le fonctionnement de cet
ordinateur de poche, à la différence du Personal computer, de
bureau ou portable, n’est plus accessible à son propriétaire :
Les PC qui sont devenus accessibles au grand public dans les
années 1980 étaient entièrement compréhensibles et program-
mables par leurs utilisateurs. Ce n’est pas le cas des nouveaux
ordinateurs mobiles, qui sont conçus de façon à interdire à
l’usager l’accès à un certain nombre de fonctionnalités et de
choix. Le problème majeur, c’est la puce dite baseband qui
se trouve au cœur de l’appareil. Toutes vos communications
avec l’extérieur - conversations téléphoniques, SMS, mails, don-
nées - transitent par cette puce. De plus en plus, ces puces
baseband sont fondues à l’intérieur même du microprocesseur ;
elles font corps avec la puce principale de l’ordinateur mobile.
Or, les spécifications d’aucune de ces puces ne sont disponibles,
si bien qu’on ne peut savoir ce qu’elles font ni les contrôler. À
l’inverse, il est potentiellement possible au fabricant ou à l’opé-
rateur d’avoir accès via ces puces à votre ordinateur1.
Le physicien Stephen Hawking signait de son côté le 1er mai
2014 une tribune dans The Indépendant avec Stuart Russel, Max
Tegmark et Frank Wilczek, où ils affirmaient que
l’intelligence artificielle peut transformer notre économie aussi
bien pour l’enrichir que pour la détruire.
Ils observaient que si nous avons sans doute tendance à
penser que,
1. Jérémie Zimmermann, « La surveillance est massive et généralisée », interview
parue dans Philosophie magazine, 19 septembre 2013.
36
LA SOCIÉTÉ AUTOMATIQUE
face à ses avantages et à ses risques possibles et incalculables,
les experts sont sans doute en train de faire tout leur possible
pour assurer le meilleur résultat,
nous avons tort. Et ils nous invitaient à prendre la mesure
des enjeux en nous adressant une question :
Si une civilisation extraterrestre supérieure nous envoyait un
message disant : « Nous arriverons dans quelques décennies »,
nous contenterions-nous de simplement répondre : « D’accord,
appelez-nous quand vous arrivez ici - nous allons laisser les
lumières allumées » ? Certainement pas. Or c’est plus ou moins
ce qui se passe avec l’intelligence artificielle.
Ils soulignaient ainsi que les enjeux sont trop graves pour ne
pas être mis prioritairement et de toute urgence au cœur de la
recherche :
Bien que nous soyons confrontés à ce qui peut constituer la
meilleure ou la pire des choses qui soit arrivée à l’humanité
au cours de son histoire, peu de recherche est consacrée à ces
questions en dehors des instituts à but non lucratif.
En se référant aux travaux de Tim O’Reilly, Evgeny Morozov
parle d’une smartification basée sur une réglementation algorith-
mique qui constitue un nouveau type de gouvernance fondé sur
la cybernétique, laquelle est avant tout la science du gouver-
nement, comme le rappelle Morozov1 - j’ai tenté de montrer
moi-même qu’elle constitue en quelque sorte et par provision
l’horizon de La République de Platon1 2.
1.	Evgeny Morozov, « La prise de pouvoir des données et la mort de la politique »,
article paru dans The Observer, traduit par Guy Weet et republié par Paul Jorion :
<http://www.pauljorion.com/blog/2014/08/25/la-prise-de-pouvoir-par-les-donnees-
et-la-mort-de-la-politique-par-evgeny-morozov/>.
2.	Cf. les cours pharmakon.fr des années 2012-2013 et 2013-2014.
37
INTRODUCTION
Morozov cite ainsi O’Reilly :
Vous saviez que la publicité s’est avérée être le modèle d’affaires
de base pour l’Internet ?... Je pense que l’assurance va être le
modèle d’affaires d’origine pour l’Internet des choses (Internet
of tbings)1.
L’idée centrale de Morozov est que, dans l’organisation
actuelle de sa collecte, de son exploitation et de ses restitutions,
ce que nous appellerons ici la rétention tertiaire numérique1
repose sur l’élimination structurelle des conflits, des désaccords
et des controverses :
La réglementation algorithmique nous offre une bonne vieille
utopie technocratique de la politique sans politique. Le désac-
cord et le conflit, selon ce modèle, sont considérés comme des
sous-produits malheureux de l’ère analogique - à résoudre par
la collecte des données - et non comme les conséquences iné-
vitables de conflits économiques ou idéologiques.
Nous verrons comment Thomas Bems et Antoinette Rouvroy
ont analysé dans des perspectives très comparables ce qu’ils
nomment eux-mêmes en référence à Michel Foucault la gou-
vernementalité algorithmique - où le business assurantiel et une
nouvelle conception de la médecine basée sur le programme
transhumaniste ont également pour but de « hacker » (c’est-
à-dire « reprogrammer ») non seulement l’Etat, mais le corps
humain1 2 3 : Google, qui soutient avec la NASA l’université de la
Singularité, investit massivement dans les technologies numé-
riques « médicales » fondées sur le calcul intensif appliqué aux
1.	Ibid.
2.	Sur le concept de rétention tertiaire numérique, cf. infra, chapitre 1, p. 65 et
suivantes.
3.	Cf. Jean-Christophe Féraud et Lucile Morin, «Transhumanisme : un corps pièces
et main-d’œuvre», Libération, 7 décembre 2014.
38
LA SOCIÉTÉ AUTOMATIQUE
données génétiques aussi bien qu’épigénétiques - et ce dans un
but explicitement eugéniste1.
9.	Le but du présent ouvrage
Morozov souligne que les activistes du Net, qui ont pris
conscience de la toxicité de « leur chose », sont cependant mani-
pulés et récupérés par la « régulation algorithmique » à travers
des organisations à but non lucratif qui ont pour finalité de
« reprogrammer l’Etat » :
Le lobby de la régulation algorithmique avance de façon plus
clandestine. Il crée des organisations à but non lucratif inof-
fensives telle « Code for America » qui a ensuite enrôlé l’Etat
sous le couvert d’encourager des hackers de talent à résoudre
les problèmes civiques.
Ces initiatives visent à reprogrammer l’État, à le rendre ouvert
au retour d’expérience et à éliminer tout autre moyen de faire
de la politique.
Morozov appelle à élaborer une nouvelle politique en matière
de technologie - où celle-ci serait mise au service d’une poli-
tique de gauche :
Alors que la plupart des créateurs de l’Internet déplorent la
façon dont leur créature est tombée aussi bas, leur colère est
mal orientée. La faute n'est pas à cette entité amorphe mais,
1.	« La société 23andme [...], filiale de Google, dirigée par la femme de Sergey
Brin, avait déposé le brevet d’une méthode qui aurait permis de fabriquer un “bébé
à la carte”, grâce à la sélection des gamètes de donneurs d’ovules et de sperme, ce
qui avait provoqué l’indignation des bioéthiciens. La start up continuera cependant
à proposer à ses clients le service d’analyse génétique familiale pour 99 dollars (80
euros), sur la base d’un échantillon de salive. » Lucile Morin et Jean-Marc Féraud,
«Transhumanisme : un corps pièces et main-d’œuvre», art. cité.
39
INTRODUCTION
tout d’abord, à l’absence d’une politique de gauche en matière
de technologie. (Je souligne.)
Nous partageons totalement cette analyse : le but de cet
ouvrage est de contribuer à établir les conditions d’une telle
politique à travers ses deux volumes consacrés à l’avenir
néguanthropique du travail et du savoir comme condition de
l’entrée dans le Néguanthropocène - où il s’agit aussi de reconce-
voir les architectures numériques et en particulier celle du Web
en vue d’une herméneutique digitale rendant aux controverses
et aux conflits d’interprétation leur valeur néguentropique, et
constituant sur cette base une économie du travail et du savoir
fondée sur l’intermittence, qui doit prendre pour modèle le
régime des intermittents du spectacle.

***

<b id="f1">1</b> : Frédéric Kaplan, «Quand les mots valent de l’or. Le capitalisme linguistique», *Le Monde diplomatique*, [Lien](http://www.monde-diplomatique.fr/201I/ll/KAPLAN/46925).[↩](#a1)

<b id="f2">2</b> : Chris Anderson, « The End of Theory. The Data Deluge Makes Scientific Method Obsolète », *Wired*, 23 juin 2008.[↩](#a2)

<b id="f3">3</b> : Que l’on désigne alors par l’expression data deltige.[↩](#a3)

<b id="f4">4</b> : Cf. « Greenspan Testimony on Sources of Financial Crisis », The Wall Street Journal, 23 octobre 2008, [Lien](http://blogs.wsj.com/economics/2008/10/23/greenspan-testimony-on-sources-of-financial-crisis/) (ma traduction avec l’aide de Google Translate).[↩](#a4)

<b id="f5">5</b> : « Un prix Nobel a été décerné pour la découverte du modèle de tarification qui sous-tend une grande partie de l’avance sur les marchés des produits dérivés », déclarait-il.[↩](#a5)

<b id="f6">6</b> : « On Chris Anderson’s The End of Theory», Edge/The Reality Club, [Lien](http://edge.org/discourse/the_end_of_theory.html)[↩](#a6)

<b id="f7">7</b> : Nassim Nicolas Taleb, *Le Cygne noir. La puissance de l'imprévisible*, Les Belles Lettres, 2008.[↩](#a7)

<b id="f8">8</b> : « Paradis fiscaux, sociétés offshore, corruption, trafics... Les responsables politiques ont beau vouloir le réformer et le moraliser, le système économique et financier mondialisé s’accommode fort bien de comportements “mafieux”. Pourquoi des relations et des formes de porosité entre économie “saine" et mafias se développent-elles ? Comment la mafia traverse-t-elle toutes les formes d’institutions ? N’est- elle pas, finalement, inhérente au capitalisme ? », Nathalie Brafman, « Mafia, stade avancé du capitalisme?», Le Monde, 15 mai 2010, <http://www.lemonde.fr/idees/article/2010/05/15/mafia-stade-avance-du-capitalisme_ 1352155_3232.html#KmbZDKpf4FxoRZmk.99>.[↩](#a8)

<b id="f9">9</b> : Ce « compromis » fordo-keynésien qui reposait lui-même sur le pillage des pays du Sud (ce qu’oublient généralement de dire les défenseurs de ce « compromis ») et qui avait conduit aux limites mises en évidence par le rapport Meadows - *The Limits to Growth*, rendu en 1972 par quatre chercheurs du MIT, Donella Meadows, Dennis Meadows, Jargen Randers et William W. Behrens III - (le pillage du Sud conduisant à l’épuisement des ressources), en France par René Passet (décrivant précisément la croissance des externalités négatives, ce que l’on apparente de nos jours aux effets hyperexponentiels de l’Anthropocène), tout en détruisant l’économie libidinale, comme nous y reviendrons dans le premier chapitre de cet ouvrage (voir p. 43 et suivantes).[↩](#a9)

<b id="f10">10</b> : Max Weber, *L'Éthique protestante et l'Esprit du capitalisme*, Pocket, coll. « Agora », 1989.[↩](#a10)

<b id="f11">11</b> : Sur ces questions, cf. Bernard Stiegler, *Mécréance et discrédit 1. La décadence des démocraties industrielles*, Galilée, 2004.[↩](#a11)

<b id="f12">12</b> : Paolo Vignola, *L'attenzione altrove. Sintomatologie di quel cbe ci accade*, Orthotes Editrice, 2013. Cf. aussi la conférence « Symptomatologie du désir », académie d’été 2013 de *pharmakon.fr*, [Lien](http://pharmakon.fr/wordpress/academie-dete-de-lecole-de-philosophie-depineuil-le-fleuriel/academie-2013/).[↩](#a12)

<b id="f13">13</b> : L’Institut de recherche et d’innovation (LRI) a exploré cette thèse en décembre 2013 au cours des Entretiens du nouveau monde industriel du Centre-Pompidou, en particulier avec Marc Giget et Michel Volle. Je reviens sur leurs analyses *infra*, p. 154 et 298.[↩](#a13)

<b id="f14">14</b> : Cf. France Stratégie, *Quelle France dans dix ans ? Les chantiers de la décennie*, Rapport au Président de la République, juin 2014, p. 36 : «Le défaut d’un objectif formulé en termes absolus est de ne pas tenir compte de la situation économique globale et européenne. Le raisonnement en termes relatifs évite cet écueil. Dans cet esprit, nous pouvons ambitionner de revenir durablement dans le premier tiers des pays européens pour l’emploi. Sachant que nous nous situons aujourd’hui vers le milieu du deuxième tiers et avons même, il y a quelques années, figuré dans le troisième, ce serait déjà une amélioration très substantielle. » [↩](#a14)

<b id="f15">15</b> : *Ibid.*, p. 35.[↩](#a15)

<b id="f16">16</b> : Jean Pisani-Ferry a été nommé commissaire général de France Stratégie le ler mai 2013.[↩](#a16)

<b id="f17">17</b> : France Stratégie, *Quelle France dans dix ans ?, op. cit.*, p. 35.[↩](#a17)

<b id="f18">18</b> : Benedikt Frey et Michael Osborne, « The Future of Employaient : How Susceptible Are Jobs to Computerisation ? », 17 septembre 2013, [Lien](http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf).[↩](#a18)

<b id="f19">19</b> : « Vous serez peut-être remplacé par un robot en 2025», 10 octobre 2014, BFMTV, [Lien](http://hightech.bfmtv.com/logiciel/vous-serez-peut-etre-remplace-par-un-robot-en-2025-839432.html?referer=app).[↩](#a19)

<b id="f20">20</b> : « Les robots vont-ils tuer la classe moyenne ? », Le Journal du dimanche, 26 octobre 2014, [Lien](http://www.lejdd.fr/Economie/Les-robots-vont-ils-tuer-la-classe-moyenne-696622).[↩](#a20)

<b id="f21">21</b> : BFM Business souligne que « le regain de productivité généré par la mécanisation de ces tâches ferait gagner 30 milliards d’euros de recettes fiscales et d’économies budgétaires, et générerait le même montant d’investissement privé, montre encore l’étude. Les entreprises débourseraient aussi 60 millions d’euros pour s’équiper en machines-employés. 13 milliards d’euros de pouvoir d’achat seraient ainsi libérés, sous forme de dividendes ou de baisse de prix. Mais, à long terme, la population serait menacée d’inactivité forcée » (*ibid.*).[↩](#a21)

<b id="f22">22</b> : Pour une reconstituüon historique et une analyse critique du concept d’Anthropocène, on pourra se reporter à l’ouvrage de Christophe Bonneuil et Jean-Baptiste Fressoz, *L’Événement Anthropocène*, Le Seuil, 2013.[↩](#a22)

<b id="f23">23</b> : Nous reprenons ainsi le titre d’un livre de Dominique Méda et Patricia Vendramin, *Réinventer le travail* (PUF, coll. « Le Lien social », 2013), avec lequel nous dialoguerons dans le chapitre 6, p. 204.[↩](#a23)

<b id="f24">24</b> : Cf. Bernard Stiegler, Mécréance et discrédit I, op. cit., §§ 23 et 32.[↩](#a24)

<b id="f25">25</b> : Christophe Bonneuil et Jean-Baptiste Fressoz, *L'Événement Anthropocène, op. cit.*, p. 32.[↩](#a25)

<b id="f26">26</b> : C’est ce que Bonneuil et Fressoz mettent en question dans leur livre (ibid., p. 68 et p. 92), et nous verrons pourquoi dans *La Société automatique*. 2. *L’Avenir du  savoir* (second volet du présent volume, à paraître fin 2015 chez Fayard). Disons, pour résumer, qu’ils montrent que, dès le début de l’Anthropocène, les conséquences de l’anthropisation industrielle font question. Mais cela est censuré par des acteurs économiques et politiques qui usent de tous les pouvoirs - notamment de lobbying, de contrôle de la presse, etc. - pour contrarier cette prise de conscience. Bonneuil et Fressoz montrent que, de nos jours, nombre de savants et de philosophes sont complices de cette dissimulation de la dimension *primordialement politique* de l’Anthropocène.[↩](#a26)

<b id="f27">27</b> : Bonneuil et Fressoz, qui rapportent le « grand récit » de l’histoire de l’industrialisation, en critiquent ensuite la simplification idéologique. Nous reviendrons sur cette critique dans *La Société automatique 2. L’Avenir du savoir*.[↩](#a27)

<b id="f28">28</b> : Christophe Bonneuil et Jean-Baptiste Fressoz, *L'Événement Anthropocène, op. cit.*, p. 173.[↩](#a28)

<b id="f29">29</b> : Cf. Bernard Stiegler, *Ce qui fait que la vie vaut la peine d'être vécue. De la pharmacologie*, Flammarion, 2010.[↩](#a29)

<b id="f30">30</b> : 3.	Cf. « Organologie », in « Vocabulaire d’Ars Industrialis » dans Bernard Stiegler, *Pharmacologie du Front national* suivi du *Vocabulaire d’Ars Industrialis*, par Victor Petit, Flammarion, 2013. Concernant l’IRI, cf. [Lien](http://www.iri.centrepompidou.fr/?lang=fr_fr/).[↩](#a30)

<b id="f31">31</b> : Christophe Bonneuil et Jean-Baptiste Fressoz, *L'Événement Anthropocène, op. cit.*, p. 83. Les « anthropocénologues » divisent l’Anthtropocène en trois étapes : la révolution industrielle, l’après-Seconde Guerre mondiale, dite « grande accélération », et la période de la thématisation de l’Anthropocène en tant que tel (cf. p. 66-69). Bonneuil et Fressoz discutent ces analyses et les contestent souvent pour les politiser, faisant ainsi de l’Anthropocène un événement proprement historique, c’est-à-dire politique. Et ils proposent une autre approche en termes de Thermocène, Thanatocène, Phagocène, Phronocène et Polémocène. Nous reviendrons sur ce travail très remarquable et fécond dans *L’Avenir du savoir*.[↩](#a31)

<b id="f32">32</b> : Face à ce que Bonneuil et Fressoz appellent le *Thanatocène* - cf. *L’Événement Anthropocène, op. cit., p. 141*.[↩](#a32)

<b id="f33">33</b> : Et Leroi-Gourhan en tirait déjà la conséquence que soulignent Bonneuil et Fressoz, à savoir qu’il n’y a pas d’unité de l’espèce humaine, cf. exergue *supra*, p. 9, et *L’Événement Anthropocène, op. cit.*, p. 89.[↩](#a33)

<b id="f34">34</b> : Erwin Schrôdinger, *Qu'est-ce que la vie ?* [1946], Le Seuil, coll. « Points Sciences », 1993, p. 170.[↩](#a34)

<b id="f35">35</b> : Francis Bailly et Giuseppe Longo, « Organisation biologique et entropie négative, à partir des réflexions de Schrôdinger », [Lien](ftp://ftp.di.ens.fr/pub/users/longo/CIM/neguentr-schr.pdf).[↩](#a35)

<b id="f36">36</b> : Mais celle-ci ne prend son sens qu’accompagnée par la grammatisation des savoir-faire telle qu’elle conduit à ce que Marx appelle l’automation dans les *Grundrisse*.[↩](#a36)

<b id="f37">37</b> : Qui est la réalité de ce que Heidegger appelle l'*Ereignis* de la «technique moderne », c’est-à-dire de la révolution industrielle, du « calcul du calculable » et de son *Gestell*, de son arraisonnement (cf. Martin Heidegger, *Questions I*, in *Questions I et II*, Gallimard, coll. « Te) », 1990, p. 274). Or, c’est précisément ce que Heidegger ne parvient pas à penser.[↩](#a37)

<b id="f38">38</b> : C’est ainsi que Heidegger qualifie le *Dasein*, c’est-à-dire « l’étant que nous sommes nous-mêmes » : il est l’étant qui a une compréhension de lui-même, et que cette compréhension, qui change avec le temps (qui est *gechichtlich*, « historiale »), et qui, comme ce changement constant, le met en question - cet être-en-question gouverne toutes ses façons d’être, y compris comme refus de « se poser des questions ».[↩](#a38)

<b id="f39">39</b> : La cosmologie rationnelle de Kant (cf. *Critique de la raison pure*, PUF, 2004) est ce qui ne peut précisément pas prendre en compte la question organologique de l’artefact, qui est au cœur de l’Anthropocène, et comme facteur à la fois entropique et néguentropique. Nous verrons dans *L'Avenir du savoir* que la prise en compte de la question organologique, qui est aussi la question pharmacologique, suppose la critique du schématisme kantien.[↩](#a39)

<b id="f40">40</b> : Sur ce sujet, cf. les cours pharmakon.fr des 10 et 17 novembre 2012, [Lien](http://pharmakon.fr/wordpress/2012-2013-cours-n°-l-10-novembre-2012) et [Lien](http://phar-makon.fr/wordpress/cours-20122013-seance-n°2-17-novembre-2012).[↩](#a40)

<b id="f41">41</b> : Qui serait le véritable *Ereignis* de ce que Heidegger appelle le *Gestell* - mais tel n’est pas le point de vue de Heidegger lui-même. Dans le « second Heidegger », l'*Ereignis* désigne un avènement qu’il appelle aussi un « tournant » (*Kehre*) dans « l’histoire de l’être », et qui est caractérisé par l’installation de ce qu’il appelle le *Gestell* (littéralement, « installation »), qui est la « situation » issue de la « technique moderne », qu’il caractérise fondamentalement par la domination de la cybernétique.[↩](#a41)

<b id="f42">42</b> : La notion d'*épokhè* est exposée à diverses reprises dans *La Technique et le Temps* (*1. La faute d’Èpiméthée,* op. cit., *2. La désorientation, Galilée*, 1996, et *3. Le temps du cinéma et la question du mal-être*, Gallilée, 2001) et en divers ouvrages, en particulier dans *Ce qui fait que la vie vaut la peine d’être vécue. De la pharmacologie (op. cit.)*. J’y reviens au paragraphe suivant et *infra*, p. 60, 68 et 135.[↩](#a42)